{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswathi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "                                                Text Language\n",
      "0   Nature, in the broadest sense, is the natural...  English\n",
      "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
      "2  The study of nature is a large, if not the onl...  English\n",
      "3  Although humans are part of nature, human acti...  English\n",
      "4  [1] The word nature is borrowed from the Old F...  English\n",
      "\n",
      "Language distribution:\n",
      "Language\n",
      "English       1385\n",
      "French        1014\n",
      "Spanish        819\n",
      "Portugeese     739\n",
      "Italian        698\n",
      "Russian        692\n",
      "Sweedish       676\n",
      "Malayalam      594\n",
      "Dutch          546\n",
      "Arabic         536\n",
      "Turkish        474\n",
      "German         470\n",
      "Tamil          469\n",
      "Danish         428\n",
      "Kannada        369\n",
      "Greek          365\n",
      "Hindi           63\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Text        0\n",
      "Language    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (adjust the file path if needed)\n",
    "df = pd.read_csv('langdetect.csv')\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check language distribution\n",
    "print(\"\\nLanguage distribution:\")\n",
    "print(df['Language'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language='English'):\n",
    "    # List of Latin-based languages where lowercasing applies\n",
    "    latin_languages = ['English', 'French', 'Spanish', 'Portugeese', 'Italian', \n",
    "                       'Sweedish', 'Dutch', 'German', 'Danish']\n",
    "    # Lowercase only for Latin-based languages\n",
    "    if language in latin_languages:\n",
    "        text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed samples:\n",
      "Language: English, Sample: nature in the broadest sense is the natural physical material world or universe\n",
      "Language: Malayalam, Sample: ഭതകപരപഞചതത മതതതതൽ സചപപകകനന പദമണ പരകത ജർമൻ Natur ഫരഞച ഇഗലഷ Nature സപനഷ Naturaleza പർചചഗസ Natureza\n",
      "Language: Hindi, Sample: वकशबदकष एक मकत शबदकष एव समनतर कष वकतब मफत कतब और उपयग समगर वककवट वभनन सभषत क सकलन वकसरत मकत सतरत समगर कमस वकमडय परकलप क मडय फइल भडर वकसमचर मकत समचर यगदनकरतओ क लए यह लख इटरनट इणटरनट वशवकश क बर म बतलत ह क वकपडय क मखय पषठ क लए वकपडय क मखय पषठ दख वकपडय क आगतक परचय क लए वकपडय क बर म पषठ दख वकपडय एक मफत वब आधरत और सहयग बहभष वशवकश ह ज गरलभ वकमडय फउनडशन स सहयग परपत परयजन म उतपनन हआ इसक नम द शबद वक wiki यह सहयग वबसइट क नरमण क एक तकनक ह यह एक हवई शबद वक ह जसक अरथ ह जलद और एनसइकलपडय encyclopedia क सयजन ह दनय भर म सवयसवक क दवर सहयग स वकपडय क करड लख लख अगरज वकपडय म लख गए ह और इसक लगभग सभ लख क वह कई भ वयकत सपदत कर सकत ह ज वकपडय वबसईट क उपयग कर सकत ह इस जनवर म जमम वलस और लर सगर क दवर पररभ पररमभ कय गय यह वरतमन म इटरनट इणटरनट पर सबस लकपरय सदरभत करय ह वकपडय क आलचक इस वयवसथत परवगरह और असगतय क दष ठहरत ह और आरप लगत ह क यह इसक समपदकय परकरय म उपलबधय पर सहमत क पकष लत ह वकपडय क वशवसनयत और सटकत भ एक मदद ह अनय आलचनओ क अनसर नकल य असतयपत जनकर क समवश और वधवसक परवत भ इसक दष ह हलक वदवन क दवर कय गए करय बतत ह क वधवसक परवत आमतर पर अलपकलक हत ह और एडरय लह न ऑनलइन पतरकरत पर पचव अनतररषटरय सगषठ म वकपडय क महततव क न कवल एक वशवकश क सनदरभ म वरणत कय बलक इस एक बर बर अदयतन हन वल समचर सरत क रप म भ वरणत कय कयक यह हल म हई घटनओ क बर म बहत जलद लख परसतत करत ह जब टइम पतरक न य You क वरष क लए परसन ऑफ द इयर क मनयत द और बतय क दनय भर म कई मलयन उपयगकरतओ क दवर इसक उपयग कय जत ह और ऑनलइन सहयग म इसक बढत सफलत क मनयत द इसन वब सवओ क तन उदहरण म वकपडय क यटयब और मइसपस क सथ रख वकपडय नयपडय Nupedia क लए एक परक परयजन क रप म शर हई ज एक मफत ऑनलइन अगरज भष क वशवकश परयजन ह जसक लख क वशषजञ क दवर लख गय और एक औपचरक परकरय क तहत इसक समकष क गय नयपडय क सथपन मरच क एक वब परटल कमपन बमस इक क सवमतव क तहत क गय इसक मखय सदसय थ जमम वलस बमस CEO और लर सगर नयपडय क एडटरइनचफ और बद क वकपडय पररभ म नयपडय क इसक अपन नयपडय ओपन कटट लइसस क तहत लइसस दय गय और रचरड सटलमन क सझव पर वकपडय क सथपन स पहल इस GNU क मफत डकयमटशन लइसस म बदल दय गय लर सगर और जमम वलस वकपडय Wikipedia क ससथपक ह जह एक ओर वलस क सरवजनक रप स सपदन यगय वशवकश क नरमण क उददशय क परभषत करन क शरय दय जत ह सगर क आमतर पर इस उददशय क पर करन क लए एक वक क रणनत क उपयग करन क शरय दय जत ह जनवर क लर सगर न नयपडय क लए एक फडर परयजन क रप म एक वक क नरमण करन क लए नयपडय मलग सच क परसतवन द वकपडय क औपचरक रप स जनवर क wwwwikipediacom पर एकमतर अगरज भष क ससकरण क रप म शर कय गय ओर इसक घषण नयपडय मलग सच पर सगर क दवर क गय वकपडय क नयटरल पइट ऑफ वय क नत क इसक पररभक महन म सकतबदध कय गय ओर यह नयपडय क पररभक पकषपतहन नत क समन थ अनयथ पररभ म अपकषकत कम नयम थ और वकपडय नयपडय स सवततर रप स करय करत थ वकपडय न नयपडय स पररभक यगदनकरत परपत कय य थ सलशडट पसटग और सरच इजन इडकसग क अत तक इसक लगभग लख और भषओ क ससकरण ह चक थ क अत तक इसक भषओ क ससकरण ह गए क अत तक और क अतम दन तक भषओ क ससकरण ह गए नयपडय और वकपडय तब तक एक सथ उपसथत थ जब पहल वल क सरवर क सथय रप स म डउन कर दय गय और इसक पठय क वकपडय म डल दय गय सतमबर क अगरज वकपडय मलयन लख क सखय क पर कर गय यह तब तक क सबस बड सकलत वशवकश बन गय यह तक क इसन यगल वशवकश क रकरड क भ तड दय जसन वरष क लए कयम रख थ एक कथत अगरज कनदरत वकपडय म नयतरण क कम और वणजयक वजञपन क आशक स सपनश वकपडय क उपयगकरत फरवर म Enciclopedia Libre क नरमण क लए वकपडय स अलग ह गए बद म उस वरष वलस न घषत कय क वकपडय वजञपन क परदरशन नह करग और इसक वबसईट क wikipediaorg म सथनतरत कर दय गय तब स कई अनय परयजनओ क समपदकय करण स वकपडय स अलग कय गय ह वकइनफ क लए कस उदसन दषटकण क आवशयकत नह हत ह और यह मल अनसधन क अनमत दत ह वकपडय स पररत नय परयजनएजस सटजडयम Citizendium सकलरपडय Scholarpedia कजरवपडय Conservapedia और गगलस नल Knolकपय उदधरण जडवकपडय क कथत समओ क सबधत करन क लए शर क गय ह जस सहकरम समकष मल अनसधन और वणजयक वजञपनपर इसक नतय वकपडय फउडशन क नरमण जन क वकपडय और नयपडय स कय गय इस सतमबर क वकपडय क टरडमरक करन क लए टरडमरक करयलय और अमरक पटट पर लग कय गय इस मरक क जनवर क पजकरण क दरज दय गय दसमबर क जपन क दवर टरडमरक सरकषण उपलबध करय गय और जनवर क यरपय सघ क दवर टरडमरक सरकषण उपलबध करय गय तकनक रप स एक सरवस मरक मरक क सकप इटरनट क मधयम स आम वशवकश क जञन क कषतर म जनकर क परवधन क लए हकपय उदधरण जड कछ उतपद जस पसतक और DVDs क लए वकपडय टरडमरक क उपयग क लइसस दन क यजनय बनय ज रह ह चतरWiki feel stupid vogvIMAGE_OPTIONSIn April the conducted a Wikipedia usability study questioning users about the editing mechanismREF START परपरक वशवकश जस एनसइकलपडय बरटनक Encyclopædia Britannica क वपरत वकपडय क लख कस औपचरक सहकरम समकष क परकरय स हकर नह गजरत ह और लख म परवरतन तरत उपलबध ह जत ह l कस भ लख पर इसक नरमत य कस अनय सपदक क अधकर नह ह और न ह कस मनयत परपत परधकरण क दवर इसक नरकषण कय ज सकत ह कछ ह ऐस वधवसपरवण पज ह जनह कवल इनक सथपत उपयगकरतओ क दवर ह सपदत कय ज सकत ह य वशष ममल म कवल परशसक क दवर सपदत कय ज सकत ह हर लख क गमनम रप म य एक उपयगकरत क अकउट क सथ सपदत कय ज सकत ह जबक कवल पजकत उपयगकरत ह एक नय लख बन सकत ह कवल अगरज ससकरण म इसक परणमसवरप वकपडय अपन अवयव क वदयत क कई गरट नह दत ह एक समनय सनदरभ करय हन क करण वकपडय म कछ ऐस समगर भ ह जस वकपडय क सपदक सहत कछ लग आकरमक आपततजनक और अशलल मनत ह उदहरण क लए म वकपडय न इस नत क धयन म रखत हए अपन अगरज ससकरण म महममद क वरणन क शमल करन क खलफ एक एक ऑनलइन यचक क असवकर कर दय वकपडय म रजनतक रप स सवदनशल समगर क उपसथत क करण पपलस रपबलक ऑफ चइन न इस वबसइट क कछ भग क उपयग क परतबधत कर दय यह भ दख वकपडय क IWF बलक वकपडय क अवयव फलरड म कनन क अधन ह वशष कपरईट कनन म जह वकपडय क सरवर क मजबन क जत ह और कई समपदकय नतय और दशनरदश इस बत पर बल दत ह क वकपडय एक वशवकश ह वकपडय म परतयक परवषट एक वषय क बर म हन चहए ज वशवकश स समबधत ह और इस परकर स शमल कय जन क यगय ह एक वषय वशवकश स समबधत समझ ज सकत ह यद यह वकपडय क शबदजल म उललखनय ह अरथत यद इसन उन मधयमक वशवसनय सरत म महतवपरण कवरज परपत कय ह अरथत मखयधर मडय य मखय अकदमक जरनल ज इस वषय क ममल स सवततर ह दसर वकपडय क कवल उस जञन क परदरशत करन ह ज पहल स ह सथपत और मनयत परपत ह दसर शबद म उदहरण क लए इस नय जनकर और मल करय क परसतत नह करन चहए एक दव जस चनत द ज सकत ह उस वशवसनय सतर क सनदरभ क आवशयकत हत ह वकपडय समदय क भतर इस अकसर verifiability not truth क रप म बतय जत ह यह इस वचर क वयकत करत ह क पठक खद लख म परसतत हन वल समगर क सचचई क जच कर सक और इस वषय म अपन खद क वयखय बनय\n",
      "Language: Tamil, Sample: வககபபடய Wikipedia ˌwɪkɪˈpiːdiə கடக wikihPEEdeeə அலலத ˌwɪkiˈpiːdiə கடக wikeePEEdeeə எனபத வணக நககறற வககமடய நறவனததன உதவயடன நடததபபடம கடடகத தகககபபடம பனமழ கடடறற இணயக கலககளஞசயமகம\n",
      "Language: Portugeese, Sample: nature é uma revista científica interdisciplinar britânica publicada pela primeira vez em de novembro de\n",
      "Language: French, Sample: si vous disposez douvrages ou darticles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici merci de compléter larticle en donnant les références utiles à sa vérifiabilité et en les liant à la section notes et références en pratique quelles sources sont attendues\n",
      "Language: Dutch, Sample: nature engels voor natuur is een brits vooral natuurwetenschappelijk tijdschrift\n",
      "Language: Spanish, Sample: nature es una de las más prestigiosas revistas científicas a nivel mundial que fue fundada por joseph norman lockyer\n",
      "Language: Greek, Sample: Δεν υπάρχει αυτή τη στιγμή λήμμα με αυτόν τον τίτλο\n",
      "Language: Russian, Sample: Nature в переводе с англ\n",
      "Language: Danish, Sample: nature er et britisk multidisciplinært videnskabeligt tidsskrift der blev udgivet første gang den november\n",
      "Language: Italian, Sample: nature è una delle più antiche ed importanti riviste scientifiche esistenti forse in assoluto quella considerata di maggior prestigio nellambito della comunità scientifica internazionale insieme a science\n",
      "Language: Turkish, Sample: Nature Fransızca ve İngilizce Nature doğa Vikipedi kullanıcıları tarafından ortaklaşa olarak birçok dilde hazırlanan özgür bağımsız ücretsiz reklamsız kâr amacı gütmeyen bir internet ansiklopedisi\n",
      "Language: Sweedish, Sample: nature är en framstående brittisk vetenskaplig veckotidskrift som började utges november och grundades av norman lockyer\n",
      "Language: Arabic, Sample: نيتشر بالإنجليزية Nature قد تعني تعديل تعديل مصدري تعديل ويكي بيانات ويكيبيديا تلفظ wikibidija وتلحن wikipidia تلفظ بالإنجليزية ˌwɪkiˈpidiə والكلمة مشتقة من مقطعين ويكي wiki وتعني بلغة هاواي بالغ السرعة والثاني بيديا pedia ومشتق من كلمة موسوعة encyclopedia ويكيبيديا هي موسوعة متعددة اللغات مبنية على الويب ذات محتوى حر تشغلها مؤسسة ويكيميديا التي هي منظمة غير ربحية\n",
      "Language: German, Sample: wir sind alle auf der suche nach schnellen wegen um fließender englisch zu sprechen\n",
      "Language: Kannada, Sample: ನವಲಲರ ಇಗಲಷನಲಲ ಹಚಚ ನರರಗಳವಗಲ ತವರತ ಮರಗಗಳನನ ಹಡಕತತದದವ ಏನಗತತದ ಎದ ನನಗ ತಳದದ ನಮಮ ಮದಳನನ ಸತತವ ಕಲವ ಪದಗಳವ ಮತತ ನವ ಈ ಸಪರಣ ಸಭಷಣಯನನ ಕವಲ ಅದಭತವದ ಅದಭತ ಅಥವ ಮಹಕವಯವನನ ಬಳಸ ಮಡಬಕಗದ ಆದದರದ ನವ ಹಗ ನಮಮ ದನದನ ಜವನದಲಲ ನವ ಬಳಸಬಹದದ ಕಲವ ಸಮರಟ ಹಸ ಇಗಲಷ ಪದಗಳನನ ಕಲಯರ ಮತತ ನಮಷಗಳಲಲ ನಮಮ ಶಬದಕಶವನನ ಹಚಚಸ\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'Text' column\n",
    "df['Text'] = df.apply(lambda row: preprocess_text(row['Text'], row['Language']), axis=1)\n",
    "\n",
    "# Drop any rows with missing values (if any)\n",
    "df = df.dropna()\n",
    "\n",
    "# Verify preprocessing by checking a few samples\n",
    "print(\"\\nPreprocessed samples:\")\n",
    "for lang in df['Language'].unique():\n",
    "    sample = df[df['Language'] == lang]['Text'].iloc[0]\n",
    "    print(f\"Language: {lang}, Sample: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature matrix shape: (10337, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "\n",
    "# Fit and transform the 'Text' column\n",
    "X = tfidf.fit_transform(df['Text']).toarray()\n",
    "y = df['Language']\n",
    "\n",
    "# Check the shape of the feature matrix\n",
    "print(\"\\nFeature matrix shape:\", X.shape)  # Should be (10337, 5000) based on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (8269, 5000)\n",
      "Testing set size: (2068, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\nModel training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.9545454545454546\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.93      0.97       106\n",
      "      Danish       1.00      0.92      0.96        73\n",
      "       Dutch       0.98      0.95      0.97       111\n",
      "     English       0.80      1.00      0.89       291\n",
      "      French       0.97      0.98      0.98       219\n",
      "      German       1.00      0.95      0.97        93\n",
      "       Greek       1.00      0.93      0.96        68\n",
      "       Hindi       1.00      1.00      1.00        10\n",
      "     Italian       1.00      0.97      0.98       145\n",
      "     Kannada       1.00      0.98      0.99        66\n",
      "   Malayalam       1.00      0.89      0.94       121\n",
      "  Portugeese       1.00      0.93      0.96       144\n",
      "     Russian       1.00      0.94      0.97       136\n",
      "     Spanish       0.96      0.97      0.97       160\n",
      "    Sweedish       0.96      0.98      0.97       133\n",
      "       Tamil       1.00      0.97      0.98        87\n",
      "     Turkish       1.00      0.86      0.92       105\n",
      "\n",
      "    accuracy                           0.95      2068\n",
      "   macro avg       0.98      0.95      0.96      2068\n",
      "weighted avg       0.96      0.95      0.96      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Define prediction function\n",
    "def predict_language(text):\n",
    "    processed_text = preprocess_text(text, 'English')  # Default to English for new input\n",
    "    text_vector = tfidf.transform([processed_text]).toarray()\n",
    "    prediction = model.predict(text_vector)\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "Text: 'hello how are you' -> Predicted Language: English\n",
      "Text: 'നിനക്ക് സുഖമാണോ' -> Predicted Language: English\n",
      "Text: 'привет как дела' -> Predicted Language: Russian\n",
      "Text: 'hola cómo estás' -> Predicted Language: Spanish\n",
      "Text: 'ನಮಸ್ಕಾರ ನೀವು ಹೇಗಿದ್ದೀರಿ' -> Predicted Language: Kannada\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Test the model\n",
    "print(\"\\nPredictions:\")\n",
    "test_texts = [\n",
    "    \"hello how are you\",           # English\n",
    "    \"നിനക്ക് സുഖമാണോ\",            # Malayalam: \"Are you well?\"\n",
    "    \"привет как дела\",             # Russian: \"Hi, how are you?\"\n",
    "    \"hola cómo estás\",             # Spanish: \"Hello, how are you?\"\n",
    "    \"ನಮಸ್ಕಾರ ನೀವು ಹೇಗಿದ್ದೀರಿ\"     # Kannada: \"Hello, how are you?\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    print(f\"Text: '{text}' -> Predicted Language: {predict_language(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Save the model and vectorizer\n",
    "joblib.dump(model, 'language_model.pkl')\n",
    "print(\"\\nModel saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)  # Should output 1.3.2 based on your logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
